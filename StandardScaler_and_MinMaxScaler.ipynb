{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StandardScaler and MinMaxScaler.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnU6aGJdXeAf",
        "colab_type": "text"
      },
      "source": [
        "**Normalization**\n",
        "\n",
        ">*Normalization scales each input variable separately to the range 0-1, which is the range for floating-point values where we have the most precision.*\n",
        "\n",
        "**Standardization**\n",
        "\n",
        ">*Standardization scales each input variable separately by subtracting the mean (called centering) and dividing by the standard deviation to shift the distribution to have a mean of zero and a standard deviation of one.*\n",
        "\n",
        "**Objective:**\n",
        ">How to use scaler transforms to standardize and normalize numerical input variables for classification and regression.\n",
        "\n",
        "\n",
        "Differences in the scales across input variables may increase the difficulty of the problem being modeled. An example of this is that large input values (e.g. a spread of hundreds or thousands of units) can result in a model that learns large weight values. A model with large weight values is often unstable, meaning that it may suffer from poor performance during learning and sensitivity to input values resulting in higher ***generalization error.***\n",
        "\n",
        "*(**Wiki Generalization error** - is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data. Because learning algorithms are evaluated on finite samples, the evaluation of a learning algorithm may be sensitive to sampling error. As a result, measurements of prediction error on the current data may not provide much information about predictive ability on new data. Generalization error can be minimized by avoiding overfitting in the learning algorithm. The performance of a machine learning algorithm is measured by plots of the generalization error values through the learning process, which are called learning curves.)*\n",
        "for more info refer: https://en.wikipedia.org/wiki/Generalization_error\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOGUf55psBDO",
        "colab_type": "text"
      },
      "source": [
        "**Popular Normalization and Standardization techniques:**\n",
        "\n",
        "*MinMax Scaler*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNJbovQXr-p_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "0ef2d545-80be-48ba-d3c9-9b886cd683b2"
      },
      "source": [
        "from numpy import asarray\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# define data\n",
        "data = asarray([[100, 0.001],\n",
        "\t\t\t\t[8, 0.05],\n",
        "\t\t\t\t[50, 0.005],\n",
        "\t\t\t\t[88, 0.07],\n",
        "\t\t\t\t[4, 0.1]])\n",
        "print(data)\n",
        "# define min max scaler\n",
        "scaler = MinMaxScaler()\n",
        "# transform data\n",
        "scaled = scaler.fit_transform(data)\n",
        "print(scaled)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0e+02 1.0e-03]\n",
            " [8.0e+00 5.0e-02]\n",
            " [5.0e+01 5.0e-03]\n",
            " [8.8e+01 7.0e-02]\n",
            " [4.0e+00 1.0e-01]]\n",
            "[[1.         0.        ]\n",
            " [0.04166667 0.49494949]\n",
            " [0.47916667 0.04040404]\n",
            " [0.875      0.6969697 ]\n",
            " [0.         1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZaWdypOslaY",
        "colab_type": "text"
      },
      "source": [
        "*Standard Scaler*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ-q4qjDs14_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c35d86cb-4781-4f67-88e6-59b35fcaba3f"
      },
      "source": [
        "from numpy import asarray\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# define data\n",
        "data = asarray([[100, 0.001],\n",
        "\t\t\t\t[8, 0.05],\n",
        "\t\t\t\t[50, 0.005],\n",
        "\t\t\t\t[88, 0.07],\n",
        "\t\t\t\t[4, 0.1]])\n",
        "print(data)\n",
        "# define standard scaler\n",
        "scaler = StandardScaler()\n",
        "# transform data\n",
        "scaled = scaler.fit_transform(data)\n",
        "print(scaled)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.0e+02 1.0e-03]\n",
            " [8.0e+00 5.0e-02]\n",
            " [5.0e+01 5.0e-03]\n",
            " [8.8e+01 7.0e-02]\n",
            " [4.0e+00 1.0e-01]]\n",
            "[[ 1.26398112 -1.16389967]\n",
            " [-1.06174414  0.12639634]\n",
            " [ 0.         -1.05856939]\n",
            " [ 0.96062565  0.65304778]\n",
            " [-1.16286263  1.44302493]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qwFm2l_tJyn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}